# Section 7: Storage and volumes

### Overview of Docker Storage drivers
- A docker image is built up from a series of layers.
- Storage Driver piece all these things together for you.
- There are multiple storage drivers available with different trade-off.

- Now whenever we create a container from an image, let's say that this is a Ubuntu image and we create a container from this specific image.
- Now after creating a container, when we log in to the container and we run a `ls` command, 
- you will be able to see that multiple files which are spanned across the layers are merged together and they are shown to you as a single output.
- This **spanned across the layers are merged together and they are shown to you as a single output** is done with the help of **storage drivers**.

- **CoW strategy: Copy on Write strategy**
- Copy on Write means everyone has a single shared copy of the same data until its written and then a copy is made.

- let's understand this with an example.
- So let's say you have a common file over here and there are two programs which needs to access the common file.
- So in such cases, what will happen is both of these programs will make use of a single common file. You will not have two files which will be created.
- Both the programs will use a single common file. Now let's say you have a program three Again, that program three wants to have access to the common file.
- Again, it will use that same common file. So in this way you have resources which will be shared and disk space will be saved.

- Now the question is what happens when a write is being made to this specific file?
- And in this case there is a little different strategy.
- So when a write operation happens within the cow strategy, what happens is that common file is being copied to the layer where the write is happening.
- So let's assume that program one is running on layer three of the Docker image, and Layer three is making some write changes to the common file.
- So now the common file which was shared over here, it will be copied to the layer three and then whatever write which was supposed to happen, that would happen at the layer three approach.

- **Watch video 200 of Zeal Vohra course. IMPORTANT**

- Let's say this file in layer three is of 500 MB and you are making a change to this specific file. So this file will be copied to your container layer completely.
- So the entire 500 MB gets copied to this container layer and the change would be made at this specific container layer.
- Now whenever a read operation happens, Docker storage driver will ensure that if the file exists in this container layer, it will not read the file in the below layer altogether.
- It will read the file from the container layer, which is the read and write layer over here.

- **CMDS:**
```docker info```

- The storage driver details will be present over here.
- During early time `aufs` was the standard storage drivers, but now it is recently changed.
- You should see the storage driver as `overlay2` and the backing file system is `xfs`. Check this.

- **CMDS:**
```
cd /var/lib/docker
ls -ltr
```

- You will see a name with directory storage driver.

- **CMDS:**
```
cd <overlay driver>
ls -ltr
cd l
ls -ltr
docker pull ubuntu
docker images
ls -ltr
```
- On executing `ls -ltr` you will see multiple layers created over here. These directories are associated with layers.
- Typically the lowest layer will have the highest size.

- **CMDS:**
```
du -sh *
```

- The above command will help you find the lowest layer with the highest size.
- take the id value and go to that directory.
- You should see `diff` and `link`
- **CMDS:**
```
cd <id value>
ls -l
cd diff
ls -ltr
du -sh *
cd ..
ls -ltr
```

- `link` is a file not a directory.
- **CMDS:**
```
vi link
```
- On opening the `link` -> you will see what is called a `short hand identifier`
```
cd ..
ls -ltr
```



- `Higher layer` is basically your `Container layer`
- `Lower layer` is basically your `Image layer`

- **Watch the video. Need to watch `video 200` multiple times**


### Block and Object storage
- **Block Storage**
- In block storage, the data is stored in terms of blocks.
- Data stored in blocks are normally read or written entirely a whole block at the same time.
- Most of the files systems are based on block devices.
- Every block has an address and application can be called via SCSI call via its address.
- There is no storage side meta-data associated with the block except the address.
- This block has no desciption, no owner.

- **Object Storage**
- Object storage is data storage architecture that manages data as objects as opposed to blocks of storage.
- An **object is defined as a data (file)** along with all its meta-data which is combined together as an object.
- AWS S3 is a type of a object based store.
- This object is given an ID which is calculated from the content of the object (from the data and metadata).
- Application can then call object with the unique object ID.

- ** Difference between Object and Block Storage **
- **Object Storage: **
- Store virtually unlimited files.
- Maintain file revisions.
- HTTP(s) based inerface. -> Meaning that you can open a file in the browser via https protocol. This is not possible with block storage.
- Files are distributed in different physical nodes.

- **Block Storage;**
- Files is split and stored in fixed sized block.
- Capacity can be increased by adding more nodes.
- Suitable for application which require high OPS, database, transactional data.

### Changing Storage drivers
- The default storage driver for docker is `overlay2`. Use `docker info`

- one very important part to remember is that if we are going to change the storage driver, then
- whatever data that we might have for the previous storage driver would be inaccessible and whatever containers which were launched would be inaccessible.

- **Note: For this exercise. Do not use EC2 instance use UBUNTU.**
- ** Amazon Linux 2 doesn’t ship AUFS support — you’d have to switch to Ubuntu or Debian for easy AUFS usage.**
- **Dont do below exrcise. Watch Video 202. Its a bit difficult as there are lot of discrepancies.

- **CMDS:**
```
docker info
sudo docker container run -dt --name overlay2 nginx
sudo docker ps
```
- **CMDS:** open a root shell, then cd
```
sudo -i 
sudo cd /var/lib/docker
ls -ltr
sudo bash -lc 'cd /var/lib/docker && ls -al'
docker info | grep -i 'Storage Driver'
```
- **CMDS:**
```
cd /var/lib/docker/overlay2
ls -ltr
docker ps
```
- **CMDS:**
```
systemctl stop docker.socket
systemctl stop docker
systemctl status docker
systemctl status docker.socket
```

- **CMDS:**
```
cd ~
cd /etc/docker/
ls -ltr
vi daemon.json
```

- **CODE: daemon.json**
```
{
  "storage-driver": "aufs"
}
```

```
systemctl start docker`
docker info
```

- when you check the `docker info`. Under the `Storage Driver` -> aufs will be present.
- What we did was to change the default storage driver `overlay2` -> `aufs` in docker.
- so when we execute commands `docker ps` and `docker ps -a` we will not be able to see the containers that were created.
- because they are made to be inaccessible.

- **CMDS:**
```
cd /var/lib/docker
ls -ltr

```
- there will be a directory of type `aufs`

- **CMDS:**
```
docker container run -dt --name aufs nginx

```
- Here when we run the above command, the image will be pulled because of the storage driver change.

- **CMDS:**
```
docker ps
cd aufs
ls -ltr

```

- this time the directory would be under the `aufs`.
- So you see there is a different approach in which stores the data.
- So all of the data that you might store in the container readwrite layer as well as the images would be part of the `aufs` directory here.


### Docker volumes
- **Challenges with files in Container Writeable Layer:**
- By default all files created inside a container are stored on a writable container layer. This means that:
- The data does not persist when that container no longer exist and it can be difficult
- to get the data out of the container if another process needs it.

- So docker has 2 options for container to store files in the host machine,
- so that the files are persisted after the container stops: **volumes** and **bind mounts**.
- If you are running Docker on Linux you can also use a tmpfs mount.

- You need to make sure that a container is running.
- **CMDS:**
```
docker info
sudo docker container run -dt --name mynginx nginx
sudo docker ps
docker ps
sudo su
cd /var/lib/docker
cd overlay2
ls -ltr
```
- So this specific container, whatever data that you might be writing inside the container would be stored in a specific directory.
- Now this directory, whatever container read and write layer that we were discussing.
- So this read and write layer, whatever things that you will be writing here.
- It depends upon the container lifecycle.
- As we discussed, **if the container is deleted, then whatever data that you might have written through the read and write layer would also cease to exist.**
- So let's try it out.

- **CMDS:**
```
ls -ltr
docker ps
docker stop mynginx
docker rm mynginx
ls -ltr

```

- Now it has been deleted. And if you want to recover data, there is no way to do that.
- And this is the reason why for the applications which are stateless.
- This type of architecture would not really matter.
- But if you have a stateful application, so let's say that many organizations, they host their databases
- like MySQL inside the container.
- Now they do not really want that, that the container, if it gets deleted or if there are any issues
- with the container, they would not be able to access the data.
- ** So you have to keep the data of the container separately and the container separately.That is the recommended approach.**
- So that can be achieved with the help of bind mounts as well as volumes.

- **CMDS:**
```
docker volume ls
docker volume create myvolume
docker volume ls
```

- So the volume is created.
- Now whatever container that we might launch, we can associate that containers directory with the volumes directory.

- **CMDS:**
```
docker container run -dt --name busybox -v myvolume:/etc busybox sh
docker ps
docker volume ls
docker volume inspect myvolume
cd /var/lib/docker/volumes/myvolume/_data/
ls -ltr
docker container exec -it busybox sh
cd /etc
ls -ltr
df -h
```

- So if you basically do a `df -h`, you will see that the root file system is based on overlay.
- And then you have the `etc` directory here. `etc` directory is within the volume of `/dev/xvda1`.
- So this is a different disk altogether when compared to the root volume.

- **CMDS:**
```
exit
cd /var/lib/docker/volumes/myvolume/_data/
docker ps
docker stop busybox
docker rm busybox
docker ps
docker ps -a
```

- So the docker container will cease to exist. But remeber we are using volumes,
- so our read/write data will be saved in the volume.

- So the container has been deleted. However, if you do an `ls -ltr` here you can see that the data continues to be present.
- So this data, which is present within the volume, is independent about the container lifecycle.
- So even if the container is deleted, the data still remains.

- **CMDS:**
```
ls -ltr
docker volume ls
cd 
docker volume rm myvolume
docker volume ls
```

** Important Pointers:**
- A given volume cane be mounted into multiple containers simultaneoulsy.
- When no running container is using a volume, the volume is still available to Docker and is not removed automatically.
- When you mount a volume, it may be named or anonymous. Anonymous Volumes are not given an explicit name when they are first
- mounted into a container, so docker gives them a random name that is guarenteed to be unique within a given Docker host.

- **Output**
- refer `Output 37` and `Output 38`

### Bind Mounts
- When you use a `bind mount`, file or directory on the host machine is mounted into a container.
- The file or directory is referenced by its full or relative path on the host machine.

- Let's say you already have a directory and there are certain contents inside that directory.
- Now you want your container to be able to access the contents within the directory inside the host.
- So in such cases you can make use of bind mounts.

- ** CMDS: **
```
docker container run -dt --name nginx nginx
docker ps
docker inspect nginx
docker container exec -t nginx bash
cd /usr/share/nginx/html/
ls -ltr
```

- You will see a default `index.html`. This is the default nginx html image.

- ** CMDS: **
```
exit
mkdir index
cd index
ls -ltr
echo "Hello, this is bind mount file" > index.html
docker container run -dt --name nginxbind01 -v /root/index/:/usr/share/nginx/html nginx
```

- Here we are running a container that specifies the directory under the host (`/root/index`)
- and i also need to specify the directory under the container which is `/usr/share/nginx/html`

- ** CMDS: **
```
docker ps
docker inspect nginxbind01
curl <IP Address of the container>
docker container exec -it nginxbind01
cd /usr/share/nginx/html/
ls -ltr
```

- This is the index.html file which came from the host directory.
- The host directory is basically the root index file.

- ** CMDS: **
```
exit
pwd
```

- Whenever you are creating a bind mount there are 2 ways to do bind mount.
- **First way**
- **CMDS:**
```

docker container run -dt --name nginxbind02 --mount type=bind,source=/root/index,target=/usr/share/nginx/html nginx
docker ps
docker inspect nginxbind02
curl <Ip address of the container present inside `Networks`>
docker container exec -it nginxbind02 bash
cd /usr/share/nginx/html
ls -ltr
echo "This is my custom changes" >> index.html
cat index.html

```

- **CMDS: Read Only bind mount**

```
docker container run -dt --name nginxbind03 --mount type=bind,source=/root/index,target=/usr/share/nginx/html nginx
docker ps
docker container exec -it nginxbind03 bash
cd /usr/share/nginx/html/
ls -ltr
cat index.html
echo "This is my 3rd change" >> index.html

```

- You will get a bash prompt saying that this is read only.

### Automatically remove columen on container exist
- Whenever a container is created with -dt flag and if the main process completes/stops, then it goes into the exited stage.
- We can see list of all containers (Running/Exited) with docker ps -a command.
- Many a times, container performs a job and we want container to automatically get deleted once it exits.
- This can be achieved with the `--rm` option.

- **CMDS:**
```
docker container run -dt --name container01 busybox ping -c10 google.com
docker ps
docker ps -a
```

- So now what we want is after the command or after the script has completed its execution, we don't
- really want it to be in the exited stage because it will unnecessarily lead to disk usage.
- So what we want is that the container should be removed automatically when the job is completed.
- So in such cases you can make use of the `--rm` option.

- **CMDS:**
```
docker container run --rm -dt --name container02 busybox ping -c10 google.com
docker ps
docker ps
docker ps -a
```

- You need to wait for few seconds for the container to finish its 10 ping execution.
- so when you add `--rm` into the container creation. Once the command finish execution, the container will be deleted thus preventing disk usage.

- **CMDS:**
```
docker container run -dt --name container02 -v /testvolume busybox ping -c10 google.com
docker volume ls
docker ps -a
```
- The problem with the above approach is that the container will cease to exit after 10 pings but still the volume will persist.

- **CMDS:**
```
docker container prune -y
docker ps -a
docker volume ls

```

- In our scenario, when we go for the volume +`--rm` approach, when the container is deleted, the volume will also to be deleted.
- **CMDS:**
```
docker volume rm <volume id>
docker container run --rm -dt --name container02 -v /testvolume busybox ping -c10 google.com
docker ps
docker volume ls
docker ps -a
docker volume ls
```
- So when we use `--rm`, when the container cease to exist, because we are using `--rm` option -> the container volume will also be deleted.


### Device Mapper
- The **device mapper is a framework** provided by the **Linux kernel** for **mapping physical block devices onto higher-level virtual block devices.**
- Device mapper works by passing data from a virtual block device, which is provided by the device mapper itself, to another block.
- Device mapper creates logical devices on top of physical block device and provide features like: RAID, Encryption, Cache and various others.

- There are 2 modes for devicemapper storage driver:
- **loop-lvm mode:**
- Should only be used in testing enviroment
- Makes use of loopback mechanism which is generally on the slower side.

- **direct-lvm mode:**
- Production hosts using the devicemapper storage driver must use `direct-lvm` mode.
- This is much more faster then the loop-lvm mode.

- **CMDS:**
```
vi /etc/docker/daemon.json
```

- **CODE:**
```
{
 "storage-driver": "devicemapper"
}
```

- **CMDS:**
```
docker info | grep -i storage
docker images
docker pull busybox
docker container run -dt --name busybox busybox sh
docker ps
```
- This container and this image is associated with storage driver.
- The storage driver that we are usings is `overlay2` and we can identify it by using the command -> `docker info | grep -i storage`

- **CMDS:**
```
systemctl restart Docker
systemctl status docker
docker info
```

- You will see a warning saying that `loop-lvm` will not be used. 
- Remember `loop-lvm` is used in testing only. It is recommended to use `direct-lvm` mode.

### Docker logging drivers
- UNIX and Linux commands typically open 3 I/O streams when they run, called `STDIN`, `STDOUT` and `STDERR`
- Similary to the above statement, docker also has similar commands.

- **STDIN (Standard Input)**
- **What it is:** The input stream, usually from the keyboard or piped input.
- **Docker use:** If you want to interactively send input to a container (like entering commands or data), you can enable STDIN.
- **Example:**
```
docker run -i ubuntu cat
```

- **STDOUT (Standard Output)**
- **What it is:** The default output stream for displaying results.
- **Docker use:** Everything printed to the terminal by the container (e.g., from echo, print, console.log) goes to STDOUT.
- **Example:**
```
docker run ubuntu echo "Hello from container"
```

- **STDERR (Standard Error)**
- **What it is:** The stream used for error messages and diagnostics.
- **Docker use:** Any error messages from the container's application are sent to STDERR.
- **Example:**
```
docker run ubuntu bash -c "ls /nonexistent"
```

**CMDS:**
```
docker ps
docker container run -dt --name mybusybox busybox ping gooogle.com
docker ps
docker logs mybusybox

```

- you will able to see the output of the commands that are running.

- **CMDS:**
```
docker info
```
- Under `Log`, you have various log drivers such as `awslogs`, `fluentd`, `gcplogs`, `gelf`, `json-file` ,`none` , `syslog` , `local` ,` journald` , `splunk`.
- These are the different type of logging drivers present in docker.

- **CMDS:**
```
docker info | grep -i "logging driver"
```
- So anytime when you create a container and you do not specify a logging driver by default, 
- Docker will associate the `json-file` logging driver with your container so we can even find it out.

- **CMDS:**
```
docker ps
docker inspect mybusybox

```
- Here you will find `LogConfig: { "Type": "json-file" ...}`
- All of your log paths of the container are stored in a specific folder. Refer `LogPath`.
- This will give you the default path in which all the logs will be stored.

- **CMDS: to see all the logs present inside the log path**
```
less <copy the log path>`
docker ps
```

- **CMDS:**
```
docker container run -dt --name mycustom --log-driver none busybox ping google.com
docker ps
docker logs mycutom

```

- The docker logs command is not available for drivers other than `json-file` and `journald`.

### Creating volumes in Kubernetes
- On-disk files in a Container are ephmeral. 
- Ephemeral here means that if the container goes down, the data becomes inaccessible or if the container gets terminated the data gets lost.
- When there are multiple containers who wants to share same data, it becomes a challenge.
- One of the benefit is that it supports multiple types of volumes


- **CMDS:**
```
eksctl create cluster --name lrm-eks-clstr --region us-east-1 --node-type t2.medium  --zones us-east-1a,us-east-1b
eksctl delete cluster --name lrm-eks-clstr --region us-east-1
```

- **CODE: pod-volume.yml **
```
apiVersion: v1
kind: Pod
metadata:
  name: demopod-volume
spec:
  containers:
  - name: test-container
    image: nginx
    volumeMounts:
    - name: first-volume
      mountPath: /data
  volumes:
  - name: first-volume
    hostPath:
      path: /mydata
      type: Directory
```

- since we know that host path basically mounts a specific file or directory within the host inside a container.
- So we specify via path and the `path` is `/mydata`. And we also tell a type the `type` is `Directory`.
- So this path that we want to mount inside the container is of a directory type.
- This is the path which is present within the host where pods would be running.

- `volumeMounts` does is that it says that you take this specific volume, which is first volume.
- So you see within the volume you have `first-volume`.
- So this `volumeMounts` is associated with the `first-volume` and then you specify the mount path.
- So where exactly this directory from, the host should be mounted inside the container.

-**CMDS:**
```
mkdir /mydata
kubectl apply -f pod-volume.yml
kubectl get pods
kubectl exec -it demopod-volume bash
cd /data
touch kplabs.txt
echo "Hi" > kplabs.txt
ls
df -h
exit

```

- This directory `/data` is mounted from the host inside the container.
- when you execute `df -h` , you will see a `Mounted on` -> you will see a `/data` partition.
- So whatever files that you store inside `/data`, those files will underlyingly be stored in the `/mydata` of the host.

-**CMDS:**
```
cd /mydata
ls -ltr
cat kplabs.txt

```

### Persistent volume and Persistent volume claim:
- A **persistentVolume(PV)** is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes.
- Every volume which is created can be of different type.
- This can be taken care by the **storage Administrator/Ops Team**.

- A **PersistentVolumeClaim** is a request for the storage by a user.
- Within the clain, user need to specify the size of the volumen along with access mode.
- **Developer:** I want a volume of size 10 GB which is has speed of Fast for my pod.

- Storage Administrator takes care of creating PV.
- Developer can raise a `Claim` (I want a specific type of PV).
- Reference that claim within the PodSpec file.

**CMDS:**
```
vi pv.yml
```

**CODE: pv.yml -> PersistentVolume -> Storage Administrator**
```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: block-pv
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /tmp/data

```

- **CMDS:**
```
mkdir /tmp/data
kubectl apply -f pv.yaml
```
- You will see the output as `persistentvolume....`

- **CMDS:**
```
kubectl get pv
```

- **CODE: pvc.yml -> PersistentVolumeClaim.yml -> Developer**
```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc
spec:
  storageClassName: manual  
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
```

- **CMDS:**
```
kubectl apply -f pvc.yml
kubectl get pvc
kubectl get pv
```

- **CODE: pod-pvc.yml**
```
apiVersion: v1
kind: Pod
metadata:
  name: kplabs-pvc
spec:
  containers:
    - name: my-frontend
      image: nginx
      volumeMounts:
        - mountPath: "/data"
          name: my-volume
  volumes:
    - name: my-volume
      persistentVolumeClaim:
        claimName: pvc	
```

- The developer will reference the claim `pvc`

- **CMDS:**
```
kubectl apply -f pod-pvc.yml
kubectl get pods
kubectl exec -it kplabs-pvc
cd /data
ls -ltr
touch kplabs.txt
exit

```

### Volume expansion in K8s:
- It can happen that your persistent volume has become full and you need to expand the storage for additional capacity.

- Lets say you have a PV and the current capacity of your PV is 10 GB and it is completely full.
- So at the later stage you might need to provision an additional capacity on top of this.
- So this is what is referred as the `volume expansion`.

- **1. Enabling Volume Expansion in the Storage Class.**
- **Step 1:**
- If you want to go ahead and expand the volume, the first step is to enable the volume expansion.
- Now you have to make sure that a parameter of allow volume expansion is set to true Associate it with
- the storage class where your PV is associated with.

- **Step 2:**
- Now once you have this parameter configured, the second step is to go ahead and perform the resizing
- of your PVC, which is the persistent volume claim.
- So within the PVC you go ahead and resize it.

- **Step 3:**
- Once PVC object is modified, you will have to restart the POD.
- kubectl delete pod [POD-NAME]
- kubectl apply -f pod-manifest.yml

- **CODE: pod-pv.yml**
```
kind: Pod
apiVersion: v1
metadata:
  name: storage-pod
spec:
  containers:
    - name: my-frontend
      image: busybox
      command:
        - sleep
        - "36000"
      volumeMounts:
      - mountPath: "/data"
        name: my-do-volume
  volumes:
    - name: my-do-volume
      persistentVolumeClaim:
        claimName: kplabs-pvc
```
- **CODE: pvc.yml**
```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: kplabs-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  storageClassName: do-block-storage
```
- ** External Commands Used:**
```
kubectl edit pvc kplabs-pvc
kubectl exec storage-pod -- sh
df -h
```

- **CMDS:**
```
kubectl get storageclass do-block-storage -o yaml
```

- On executing the above command we need to check if the `allowVolumeExpansion:true`
- because we are expanding volumen this step has to be true.


- Under pvc.yml -> name -> kplabs-pv -> we have a storage of 5Gi

- Under pv.yml -> we have the pod on the image of `busybox`. Here we are making use of `persistentVolumeClaim` ->`kplabs-pvc`
- and we are mounting it on mountPath ->`/data`

- **CMDS:**
```
ls -ltr
kubectl apply -f pvc.yml
kubectl get pvc
kubectl get pv
kubectl apply -f pod-pv.yml
kubectl get pods
kubectl describe pod storage-pod
kubectl exec -it storage-pod -- sh
df -h
kubectl get pvc
kubectl get pv
```

- Inorder to expand the volume we have to do the following

- **CMDS:**
```
kubectl edit pvc kplabs-pvc
```

- you need to go down and edit the storage from `5Gi` to `10Gi`.

- **CMDS:**
```
:wq
kubectl get pvc kplabs-pvc -o yaml
```

- Here you will see a message -> `Waiting for user to (re-)start a pod to finish file system resize of volume on node.`
- type -> `FileSystemResizePending`

```
kubectl get pv
kubectl exec -it storage-pod -- sh
df -h

kubectl delete -f pod-pv.yml
kubectl apply -f pod-pv.yml
kubectl get pods
kubectl exec -it storage-pod -- sh
df -h
```
- This time you will see the capacity has been increased.


### Reclaim Policy:
- The reclaim policy is responsible for what happens to the data in persistent volume when 
- the kubernetes persistent volumen claim has been deleted.

- 3 types of reclaim Policy that we need to be aware of.
- **Retain**:
- When the PersistentVolumeClaim is deleted, the PersistentVolume still exists and the volume is considered "released"
 
- **Delete**:
- The persistent volumen is deleted when the claim is deleted.

- **Recycle**:
- If supported by the underlying volume plugin, the recyle reclaim policy performs a basic scrub (rm -rf /thevolume/*)
- on the volume and makes it available again for a new claim.


- The below example is delete Retain Policy.
- **CODE: pvc-pv.yml**
```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: kplabs-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  volumeName: "pvc-31a92a51-e693-41b4-ad9c-6c53329f98f2"
  storageClassName: do-block-storage
```

- **CMDS:**
```
kubectl apply -f pvc-pv.yml
kubectl get pvc
kubectl get pv

```
- Now, if you look into the volume, the size is five, and there is also an associated reclaim policy.
- And the reclaim policy that is associated with it is delete.
- So that basically means that if you go ahead and delete this PVC, the associated PVC will also be removed.

- **CMDS:**
```
kubectl delete pvc kplabs-pvc
kubectl get pv
```

### Understanding Retain Reclaim Policy:
- When PVC is deleted, the PersistentVolume still exists and the volume is considered `released`
- It is not yet available for another claim because the previous claimant's data remains on the volume.

- ** Steps for Reclaimation**
- An administrator can manually reclaim the volume with the following steps.

- Delete the PersistentVolume, The associated storage asset in external infrastructure (such as an AWS EBS, GCE PD, Azure Disk or cinder volume)
- still exists after the PV is deleted.

- Manually clean up the data on the associated storage asset accordingly.

- Manually delete the associated storage asset, or if you want to reuse the same storage asset,
- create a new PersistentVolume with the storage asset definition.

- **CODE: pvc-pv.yml**
```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: kplabs-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  volumeName: ""
  storageClassName: do-block-storage
```

- **CMDS:**
```
kubectl apply -f pvc-pv.yml
kubectl get pvc
kubectl get pv
kubectl edit pv <name of the pv>
```

- Here you will see a `persistentVolumeReclaimPolicy` as `Delete`.
- Modify from `Delete` to `Retain`

- **CMDS:**
```
kubectl get pv
kubectl delete -f pvc.yml
kubectl get pv
```
- When you execute the `kubectl get pv`.
- Here the reclaim policy is `Retain` and the status is `Released`

- **CODE: pvc-pv.yml**
```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: kplabs-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  volumeName: "<paste the name of the pv here>"
  storageClassName: do-block-storage
```
- **CMDS:**
```
kubectl apply -f pvc-pv.yml
kubectl get pvc
kubectl get pvc
kubectl get pv
```
- Now, if you quickly do a cube, Ctl get PVC, you will see that the status is pending and the capacity is zero.
- So even after some amount of time you will refresh, you will see that it is always on the pending state,
- primarily because the PV that is being associated is still in the release state and there is a claim
- that is associated with this PV.

- **CMDS:**
```
kubectl edit pv <Name of the pv>
```

- You will have a section called claimRef and this claim reference was associated with the previous PVC that we had created and also deleted.
- Remove the `claimRef` completely and save it.

- **CMDS:**
```
wq!
kubectl get pvc
```

- You will see the status as `Bound`

- **CMDS:**
```
kubectl delete -f pvc-pv.yml
```

- So we already know that if we delete the PVC, the status of the associated PVC would be released.

- **CMDS:**
```
kubectl delet pv <name of the pv>

```

### Mastering K8s stroage classes.
- A storageClass provides a way for administrator to describe the "classes" of storage they offer.

- Different classes might map to quality of service levels, or to backup policies, or to 
- arbitrary policies determined by the cluster administrators.

- **CODE:**
```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
 name: managed-disk
spec:
 accessModes:
 - ReadWriteOnce
 storageClassName: managed-premium
 resources:
   requests:
     storage: 5Gi
```

- So if you look into this sample diagram, there are multiple classes of storage over here.
- One is standard storage. You can have premium storage and so on.
- So depending upon this, a persistent volume might fetch the storage from a premium storage or it might fetch it from the standard storage as well.
- So the type of disk that is needed can be defined within the storage class.
- Now if you look into the sample code associated with a persistent volume claim here, you can define a storage class name.

- ** Storage Class Resource **
- Each StorageClass contains the fields **provisioner** , **parameters** and **reclaimPolicy** which are 
- used when PersistentVolume belonging to the class needs to be dynamically provisioned.

- **Understanding Provisioner**
- Each StorageClass has a provisioner that determines what volume plugin is used for provisioning PVs. The field must be specified.
- `AWSElasticBlockStore`, `AzureFile`, `Local`, `StorageOS`, `Glusterfs`

- ** CMDS: **
```
kubectl get storageclass

```

- So what happens here is that within the PVC, if you do not specify a storage class, `do-block-storage (default)` this one will be used.


- ** CMDS: **
```
kubectl get storageclass -o yaml
```

- So in case you have multiple storage class, let's assume you have one storage class for premium, one
- storage class for standard, and for the PVC, you want to request the premium storage class.
- So within the storage class name, you can specify the name of the storage class or tomorrow for a different
- application, you need a standard disk so you can change the storage class name here from managed premium
- to maybe manage standard.